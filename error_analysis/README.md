# AutoUP Automatic Proof Iteration

This repo contains the components for a workflow that use ChatGPT 4o iterate on a harness until it is able to resolve an error.

You MUST have your OpenAI API key stored as an environment variable called OPENAI_API_KEY in order to use this system.

# Directory Structure

### run_test_suite.py

The prefered way to run this script. Details on it's use can be found in the testing section below.

### llm.py

Defines a class called "ProofWriter" that is the heart of this system. This class is responsible for pretty much everything to do with the proof debugging, including making the API calls to OpenAI, adding the new preconditions to the harness, and checking if it actually resolved the error successfully.

### parser.py

This code will run a specified harness using `make` and parses through the files generated by CBMC-viewer to extract the errors found in the latest harness run and the definitions of each function relevant to our harness. It saves these to files in the `./payload` directory, which later get loaded when building the prompt for the LLM.

### output_models.py

This contains the class definitions used with OpenAI's "structued output format" feature, which requires the LLM to fill a set of output fields in it's response. 

The various classes here are useful for what we call "Show Your Work" prompting, where we include a certain output field in order to push the reasoning of the LLM in a certain direction. The output format acts like a math teacher that forces you write down each step of an equation before giving an answer, which helps make sure you do the problem correctly.

For example, if we want to ensure that the LLM does not try use variables that do not exist in a precondition (which would cause an error), we can include a field in the output where the LLM must list out the variables defined in the harness function. This basically helps the LLM "remember" what variables it can use. While most of the fields in the response are not directly useful, they are extremely effective at reducing hallucinations and ensuring more consistant outputs from the LLMs.

### advice.py

This just contains a function that returns the step-by-step instructions for resolving a particular type of error, which is used in the prompt to the LLM. This doesn't need to be it's own file, it's just convenient to keep it in a seperate file.

# Testing

Currently, the best way to run the code is through `run_test_suite.py`. The basic way it works is it reads the harness files listed in `test_config.json`, extracts the lines listed out (which should usually be __CPROVER_assume statements), and then calls the proof writer.

By default it will run all of the harnesses in `test_config.json`, but you can run just specific harnesses can be ran by passing them in under the --harnesses flag.

Once the test run is complete, the test results will be automatically formatted into a viewable report that is saved as `index.html` in the results directory. This should also automatically launch a browser window that renders the test results, although I've found the code to open this page automatically is a little bit buggy. If you want to view the report at some other time, go into the `AutoUP/error_analysis/results` directory and call `python3 -m http.server 8000` and the report should render. The code that generates the HTML report was generated by Claude 4, and there may also be unexpected bugs or visuals as a result.

